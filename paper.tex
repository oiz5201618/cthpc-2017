\documentclass[conference]{IEEEtran}
\usepackage[english]{babel}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{url,hyperref,graphicx,float,times}
%\usepackage{sectsty}
%\usepackage{authblk}
\usepackage{textcomp}
\usepackage{cite}
% \graphicspath{{../pdf/}{../jpeg/}}
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage[caption=false,font=footnotesize]{subfig}

\usepackage{url}

\begin{document}

\title{Title}

\author{\IEEEauthorblockN{Ching-Chun (Jim) Huang} \IEEEauthorblockA{Department of\\Computer Science and Information
Engineering\\National Cheng Kung University, Taiwan\\ No.1, University Road, Tainan City 701, Taiwan (R.O.C.)\\ Email:
jserv@ccns.ncku.edu.tw} \and \IEEEauthorblockN{Chung-Fan Yang} \IEEEauthorblockA{Department of Electrical Engineering\\
National Cheng Kung University, Taiwan\\ No.1, University Road, Tainan City 701, Taiwan (R.O.C.)\\ Email:
E24026048@mail.ncku.edu.tw}}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\section{Related Work}

\section{Methodology}

    Based on the works presented above, we have methods to measure the overall wake-up latency and context-switch
    latency, using cyclictest and lmbench. Also, it's able to show that the CFS algorithm has O(1) complexity on
    selecting next task, giving static latency. Thus, overall  and parts of latency of a Linux system can be determined,
    but the measurement methodology and tool of scheduler latency, shown in figure \ref{sec:latency_timeline}, are
    lacking. Therefore, we developed a evaluation tool of scheduler duration jitter, based on methodology method of A
    Decade of Wasted Cores \cite{Lozi:2016:LSD:2901318.2901326}. This methodology method enabled us to record the
    scheduler states including, executing process ID (PID), PID in scheduler run-queue, context switch points, timer
    interrupt execution, scheduling events, etc. on per processor core basis. This methodology method differs from
    profiling based methodology tools, e.g. perf \cite{perf}, lacking high fidelity, all-recorded profiling result of
    the target system. Our proposed tool can do full-detailed record in microsecond scale on x86-64 and ARM system,
    which is able to capture the impact to real-time performance of occasional short-burst scheduler events, which only
    last less that 10us. These events are mostly not shown in the result of sampling based measurement which focus on
    average-case execution time(ACET). By analyzing full-detailed record, we are able to not only capture the worst-case
    execution time (WCET), but also understand the cause of these incidents.

    In addition, the behavior and loading characteristic of typical work-load used during test in above works, e.g.
    hackbench, stress \cite{rt-tests}, etc. are far from the real-world real-time applications. These problems not only
    would reduce the significant of target system jitter measurement, but also will make incidents, harming the
    real-time performance, obscure. In result of this frustration, we also created a testing framework hosing an
    algorithm which resembled a real-time robot control system execution behavior, named mctest. This work-load
    application has the ability to be compiled either as user-space program or kernel module, provides observation of
    the characteristic of target application execution in both user-space and kernel-space. This would gives the insight
    of system-call and user-space I/O overhead, which does not exist in kernel module. In addition, mctest would do a
    series of identically executions and record the execution time of each turn. These records can be used to determined
    the execution jitter of the application on the real-time system, which besides wakeup latency. This could be used to
    reflect to the system-call overhead of the system. We have been utilizing mctest as the testing work-load and jitter
    measurement tool during methodology.

    \begin{figure} \centering \includegraphics[width=3.0in]{img/f1.png} \caption{Task Wake-up Latency}
    \label{fig:latency_timeline} \end{figure}

\section{Experiment}

    In aid of these developed tools and work presented above, we did multiple experiments and measurement of 2 ARM
    Cortex-A9 based platform, shown in table I. These platforms are selected as CMP based ARM system are emerging and
    being used in a wide field of applications. These systems are running buildroot \cite{buildroot} built Linux with
    kernel version 4.4 and PR\textunderscore RT \cite{preemptrt} patches.

    \begin{table*}[] \centering \caption{Experiment Platforms} \begin{tabular}{|l|c|c|} \hline Platform Name & Altera
        SoC FPGA Development Kit & Freescale i.mx6 Sabre \\ \hline Processor & 2x ARM Cortex-A9 & 4x ARM Cortex-A9 \\
        \hline Memory & 1 GB & 1 GB \\ \hline Additional configuration & None & \begin{tabular}[c]{@{}c@{}}L2 Cache
        locked down to CPU1,\\ Isolated CPU1,\\ Enabled tickless on CPU1 \\ Real-time task pinned to CPU1 \end{tabular}
    \\ \hline \end{tabular} \end{table*}

    Our experiment begins with known tuning practices which includes processor isolation \cite{isolatedcpu}, tickless
    feature of kernel \cite{tickless}, L2 cache lockdown. We compared the measurement result of mctest with and without
    these tuning techniques on Freesacle i.mx6 Sabre platform. Measurement results are shown in figure 2 to 4, verifying
    the effectiveness of these known tuning techniques and providing results as a baseline. We are able to see the WECT
    with and without tuning are 100us and 100us. This gives us the hint that the scheduling  would harms the real-time
    performance. This scheduling overhead could be separated into 2 parts, scheduler overhead and cache contention on
    SMP system. With L2 Cache lockdown technique we are able to mitigate most of the cache contention, but still
    resulting some degree of jitter about 100us
    
\subsection{System Call Overhead and Kernel Mode Linux}

    In addition, we compared the user-space mctest result with kernel space measurement, shown in figure 6 and 7 with
    WECT of 50us and 50us. With these result, we are able to identify the sources of jitter are system calls. To
    mitigate these, we first proposed to rewrite real-time applications as kernel modules. But this is tedious and
    making the debugging process difficult. Thus, we proposed a second approach, Kernel Mode Linux(KML) \cite{KML}
    \cite{KMLConf}. KML enables the ability for specified user-space program running in kernel memory space. Thus,
    system calls become jump instructions, not instruction trapping the processor. Also, this would mitigate the cache
    contention while switching in and out the kernel during system call. In addition, we have also rewired system calls,
    which have vDSO implementations \cite{vDSO}, to use vDSO implementation via KML. The result of user-space program
    running in KML is shown in figure 8 with WECT of 20us, which proving the effectiveness of jitter mitigation from
    KML.
    
\subsection{Scheduler Overhead Measurement}
    
    Identifying that scheduler is also a source of jitter, we applied our proposed scheduler profiling tool to evaluate
    and identify of which part of the scheduling process is the source of jitter. Knowing the IRQ latency, and
    context-switching latency, we would like to evaluate the scheduler duration. Traditionally, the latency of this part
    is considered static with the CFS algorithm complexity, designed to be O(1), but real measurement result are
    lacking. Thus, we set our proposed profiler to record the increment of scheduler run-queue and scheduler
    context-switching events with switched-to PID of each processors. A analyzer will process the recorded data and
    gives out the scheduler duration of a specified process, enabling us the understand the scheduling of real-time task
    in SMP system. The result of running a solo instance of cyclictest on SoC FPGA Development Kit is shown in figure 9.
    The latency could be considered static with maximum 10us. This result matches traditional considerations, but with
    the interference from other work-load, the jitter will start to emerge.

\subsubsection{Overloaded System}

    We created additional work-load, executing alone with cyclictest during profiling session. We created a mild loading
    condition, which has 3 stress threads, and a heavy loading condition, which has 8 stress threads. The result of mild
    loading condition is shown in figure 10. This result also exhibits a mostly static scheduler duration with seldom
    jitter with maximum of 20us. But putting the system in to heavy loading condition, the result, shown in figure 11
    exhibits random jitter from 10us to 30us. The spread of distribution of scheduler duration, shown in figure 12,
    would become wilder. This informs us that the scheduler duration is not a static value and will increase alone with
    the system's load. Also informs that the scheduler duration contains but not only contains CFS algorithm task
    selection process which is proved to be O(1).

\subsubsection{Short Inter-Arrival Time}

    We also captured large jitter of scheduler duration on a mild loaded system. The experiment is running mctest with
    cyclictest. We pinned kernel module mctest to use first processor, causing cyclictest real-time thread and main
    thread being scheduler onto second processor. On most of the time, this execution schema results the same result as
    cyclictest executing solely in the system. But on some occasions, the cyclictest main thread would be waken-up and
    enter run-queue with short inter-arrival time against the cyclictest real-time thread, which was also being
    waken-up. When this event happened, the scheduler duration will raise to 30us. This result, shown in figure 13 hints
    us that not only long term overload system would exhibit large scheduler duration, but also short burst a
    overloading condition during the real-time task scheduling and execution would cause the scheduler duration to
    raise.

\section{Conclusion}

    We have evaluated the real-time behavior of Linux by profiling kernel scheduler and measuring the latency of various
    kernel variants. An intensive interrupt load can cause long OS latencies due to the design of the interrupt
    processing mechanism. We proposed new tools to visualize task scheduling in fine-grained scale(microsecond level).
    This enabled us not only focusing on interrupt latency, but also scheduler durations, lock, and etc. It would thus
    be highly desirable to combine existing techniques, e.g KML, isolated CPU, tickless kernel, to improve task
    responsiveness under various target application characteristics, on top of PREEMPT\textunderscore RT.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
